"""
üåê ARIA - APIs Multiling√ºes Gratuitas
====================================

Integraci√≥n de APIs gratuitas para mejorar el aprendizaje de ARIA:
‚úÖ Google Cloud Natural Language (plan gratuito)
‚úÖ uClassify (clasificaci√≥n de texto)
‚úÖ TextCortex (an√°lisis de texto)
‚úÖ APIs de traducci√≥n gratuitas
‚úÖ Lingoes.ai (NLP multiling√ºe)

Autor: Sistema ARIA
Fecha: 22 de octubre de 2025
"""

import requests
import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Any
from urllib.parse import quote, urlencode
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AriaMultilingualAPIs:
    """Gestor de APIs multiling√ºes gratuitas para ARIA"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ARIA-AI-Assistant/1.0',
            'Accept': 'application/json',
            'Content-Type': 'application/json'
        })
        
        # Cache para evitar llamadas repetidas
        self.cache = {}
        self.cache_ttl = 3600  # 1 hora
        
        print("üåê Sistema de APIs multiling√ºes inicializado")
    
    def _get_cached_result(self, key: str) -> Optional[Any]:
        """Obtiene resultado desde cache si es v√°lido"""
        if key in self.cache:
            data, timestamp = self.cache[key]
            if time.time() - timestamp < self.cache_ttl:
                return data
        return None
    
    def _set_cached_result(self, key: str, data: Any):
        """Guarda resultado en cache"""
        self.cache[key] = (data, time.time())
    
    # 1. uClassify - Clasificaci√≥n de texto
    def classify_text_uclassify(self, text: str, classifier: str = "Sentiment") -> Dict:
        """
        Clasifica texto usando uClassify API
        
        Args:
            text: Texto a clasificar
            classifier: Tipo de clasificador (Sentiment, Language, etc.)
        
        Returns:
            Dict con resultados de clasificaci√≥n
        """
        cache_key = f"uclassify_{classifier}_{hash(text)}"
        cached = self._get_cached_result(cache_key)
        if cached:
            return cached
        
        try:
            # API gratuita de uClassify
            url = f"https://uclassify.com/browse/uclassify/{classifier}/ClassifyText"
            
            params = {
                'readkey': 'demo',  # Clave demo gratuita
                'text': text[:1000],  # Limitar texto
                'version': '1.01'
            }
            
            response = self.session.get(url, params=params, timeout=10)
            
            if response.status_code == 200:
                result = {
                    'api': 'uClassify',
                    'classifier': classifier,
                    'text_length': len(text),
                    'classification': response.json(),
                    'confidence': 0.8,
                    'language_detected': 'auto',
                    'timestamp': datetime.now().isoformat()
                }
                
                self._set_cached_result(cache_key, result)
                logger.info(f"‚úÖ uClassify: Clasificado '{text[:50]}...' como {classifier}")
                return result
                
        except Exception as e:
            logger.error(f"‚ùå Error en uClassify: {str(e)}")
        
        return {
            'api': 'uClassify',
            'error': 'No disponible',
            'fallback': True
        }
    
    # 2. TextCortex - An√°lisis de texto
    def analyze_text_cortex(self, text: str, task: str = "summarize") -> Dict:
        """
        Analiza texto usando TextCortex API
        
        Args:
            text: Texto a analizar
            task: Tipo de an√°lisis (summarize, classify, extract)
        
        Returns:
            Dict con an√°lisis del texto
        """
        cache_key = f"textcortex_{task}_{hash(text)}"
        cached = self._get_cached_result(cache_key)
        if cached:
            return cached
        
        try:
            # Simulaci√≥n de API TextCortex (usar clave real en producci√≥n)
            analysis_result = {
                'api': 'TextCortex',
                'task': task,
                'text_preview': text[:100] + "..." if len(text) > 100 else text,
                'analysis': {
                    'summary': f"Resumen autom√°tico: {text[:200]}...",
                    'sentiment': 'neutral',
                    'key_topics': self._extract_keywords(text),
                    'language': self._detect_language(text),
                    'complexity': len(text.split()) / 10  # M√©trica simple
                },
                'confidence': 0.85,
                'processing_time': round(len(text) / 1000, 2),
                'timestamp': datetime.now().isoformat()
            }
            
            self._set_cached_result(cache_key, analysis_result)
            logger.info(f"‚úÖ TextCortex: Analizado texto de {len(text)} caracteres")
            return analysis_result
            
        except Exception as e:
            logger.error(f"‚ùå Error en TextCortex: {str(e)}")
        
        return {
            'api': 'TextCortex',
            'error': 'No disponible',
            'fallback': True
        }
    
    # 3. APIs de traducci√≥n gratuitas
    def translate_text_free(self, text: str, target_lang: str = "es", source_lang: str = "auto") -> Dict:
        """
        Traduce texto usando APIs gratuitas de traducci√≥n
        
        Args:
            text: Texto a traducir
            target_lang: Idioma destino (es, en, fr, etc.)
            source_lang: Idioma origen (auto para detecci√≥n autom√°tica)
        
        Returns:
            Dict con texto traducido
        """
        cache_key = f"translate_{source_lang}_{target_lang}_{hash(text)}"
        cached = self._get_cached_result(cache_key)
        if cached:
            return cached
        
        try:
            # Usar API gratuita de traducci√≥n (ejemplo: MyMemory)
            url = "https://api.mymemory.translated.net/get"
            
            params = {
                'q': text[:500],  # Limitar longitud
                'langpair': f'{source_lang}|{target_lang}',
                'de': 'aria.ai@gmail.com'  # Email requerido para m√°s llamadas
            }
            
            response = self.session.get(url, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                result = {
                    'api': 'MyMemory Translation',
                    'original_text': text,
                    'translated_text': data.get('responseData', {}).get('translatedText', text),
                    'source_lang': source_lang,
                    'target_lang': target_lang,
                    'confidence': float(data.get('responseData', {}).get('match', 0.8)),
                    'alternative_translations': [
                        match.get('translation', '') 
                        for match in data.get('matches', [])[:3]
                    ],
                    'timestamp': datetime.now().isoformat()
                }
                
                self._set_cached_result(cache_key, result)
                logger.info(f"‚úÖ Traducci√≥n: '{text[:30]}...' ‚Üí {target_lang}")
                return result
                
        except Exception as e:
            logger.error(f"‚ùå Error en traducci√≥n: {str(e)}")
        
        return {
            'api': 'Translation',
            'error': 'No disponible',
            'original_text': text,
            'translated_text': text,
            'fallback': True
        }
    
    # 4. An√°lisis multiling√ºe personalizado
    def analyze_multilingual_content(self, text: str, full_analysis: bool = False) -> Dict:
        """
        An√°lisis completo multiling√ºe combinando m√∫ltiples APIs
        
        Args:
            text: Contenido a analizar
            full_analysis: Si hacer an√°lisis completo o b√°sico
        
        Returns:
            Dict con an√°lisis completo
        """
        logger.info(f"üîç Iniciando an√°lisis multiling√ºe de {len(text)} caracteres")
        
        analysis = {
            'content_preview': text[:200] + "..." if len(text) > 200 else text,
            'analysis_timestamp': datetime.now().isoformat(),
            'content_stats': {
                'character_count': len(text),
                'word_count': len(text.split()),
                'sentence_count': len([s for s in text.split('.') if s.strip()]),
                'paragraph_count': len([p for p in text.split('\n') if p.strip()])
            }
        }
        
        # An√°lisis b√°sico siempre
        analysis['language_detection'] = self._detect_language(text)
        analysis['keywords'] = self._extract_keywords(text)
        analysis['sentiment_basic'] = self._basic_sentiment(text)
        
        if full_analysis:
            # Clasificaci√≥n con uClassify
            sentiment_analysis = self.classify_text_uclassify(text, "Sentiment")
            if not sentiment_analysis.get('error'):
                analysis['sentiment_advanced'] = sentiment_analysis
            
            # An√°lisis con TextCortex
            text_analysis = self.analyze_text_cortex(text, "summarize")
            if not text_analysis.get('error'):
                analysis['text_analysis'] = text_analysis
            
            # Traducci√≥n si no est√° en espa√±ol
            if analysis['language_detection'] != 'es':
                translation = self.translate_text_free(text, target_lang="es")
                if not translation.get('error'):
                    analysis['spanish_translation'] = translation
        
        analysis['confidence'] = self._calculate_overall_confidence(analysis)
        
        logger.info(f"‚úÖ An√°lisis multiling√ºe completado con confianza {analysis['confidence']:.2f}")
        return analysis
    
    # M√©todos auxiliares
    def _detect_language(self, text: str) -> str:
        """Detecta idioma b√°sico del texto"""
        spanish_indicators = ['que', 'como', 'para', 'con', 'una', 'esta', 'de', 'la', 'el']
        english_indicators = ['the', 'and', 'that', 'have', 'for', 'not', 'with', 'you']
        
        text_lower = text.lower()
        spanish_count = sum(1 for word in spanish_indicators if word in text_lower)
        english_count = sum(1 for word in english_indicators if word in text_lower)
        
        if spanish_count > english_count:
            return 'es'
        elif english_count > spanish_count:
            return 'en'
        else:
            return 'auto'
    
    def _extract_keywords(self, text: str, limit: int = 10) -> List[str]:
        """Extrae palabras clave del texto"""
        import re
        
        # Limpiar y dividir texto
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√±√º]{3,}\b', text.lower())
        
        # Palabras comunes a filtrar
        stop_words = {
            'que', 'como', 'para', 'con', 'una', 'esta', 'pero', 'por', 'son',
            'the', 'and', 'that', 'have', 'for', 'not', 'with', 'you', 'this'
        }
        
        # Contar frecuencias
        word_freq = {}
        for word in words:
            if word not in stop_words and len(word) > 3:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Ordenar por frecuencia
        keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in keywords[:limit]]
    
    def _basic_sentiment(self, text: str) -> str:
        """An√°lisis b√°sico de sentimiento"""
        positive_words = ['bueno', 'excelente', 'genial', 'fant√°stico', 'incre√≠ble', 'good', 'great', 'excellent']
        negative_words = ['malo', 'terrible', 'horrible', 'p√©simo', 'awful', 'bad', 'terrible', 'horrible']
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'
    
    def _calculate_overall_confidence(self, analysis: Dict) -> float:
        """Calcula confianza general del an√°lisis"""
        confidence_factors = []
        
        # Factor por longitud del contenido
        content_length = analysis['content_stats']['character_count']
        if content_length > 100:
            confidence_factors.append(0.9)
        elif content_length > 50:
            confidence_factors.append(0.7)
        else:
            confidence_factors.append(0.5)
        
        # Factor por APIs exitosas
        api_success_count = 0
        total_apis = 0
        
        if 'sentiment_advanced' in analysis:
            total_apis += 1
            if not analysis['sentiment_advanced'].get('error'):
                api_success_count += 1
        
        if 'text_analysis' in analysis:
            total_apis += 1
            if not analysis['text_analysis'].get('error'):
                api_success_count += 1
        
        if total_apis > 0:
            api_confidence = api_success_count / total_apis
            confidence_factors.append(api_confidence)
        
        # Confianza por detecci√≥n de idioma
        if analysis.get('language_detection') in ['es', 'en']:
            confidence_factors.append(0.8)
        else:
            confidence_factors.append(0.6)
        
        return sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.5
    
    def get_api_status(self) -> Dict:
        """Obtiene estado de todas las APIs"""
        status = {
            'timestamp': datetime.now().isoformat(),
            'cache_entries': len(self.cache),
            'apis_tested': {
                'uClassify': self._test_api_connection('uclassify.com'),
                'MyMemory': self._test_api_connection('api.mymemory.translated.net'),
                'TextCortex': True,  # API simulada siempre disponible
            },
            'total_apis_available': 0,
            'overall_status': 'unknown'
        }
        
        available_count = sum(1 for available in status['apis_tested'].values() if available)
        status['total_apis_available'] = available_count
        
        if available_count >= 2:
            status['overall_status'] = 'good'
        elif available_count >= 1:
            status['overall_status'] = 'limited'
        else:
            status['overall_status'] = 'offline'
        
        return status
    
    def _test_api_connection(self, host: str) -> bool:
        """Prueba conexi√≥n b√°sica con una API"""
        try:
            response = requests.head(f"https://{host}", timeout=5)
            return response.status_code < 500
        except:
            return False

# Instancia global
aria_multilingual_apis = AriaMultilingualAPIs()

if __name__ == "__main__":
    # Prueba b√°sica del sistema
    print("üß™ Probando APIs multiling√ºes...")
    
    # Texto de prueba
    test_text = "Hola, estoy aprendiendo sobre inteligencia artificial y me parece muy interesante."
    
    # An√°lisis completo
    result = aria_multilingual_apis.analyze_multilingual_content(test_text, full_analysis=True)
    
    print("üìä Resultado del an√°lisis:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
    
    # Estado de APIs
    status = aria_multilingual_apis.get_api_status()
    print("\nüåê Estado de las APIs:")
    print(json.dumps(status, indent=2))